
@inproceedings{ju_automatic_2020,
	title = {Automatic {Chord} {Labelling}: {A} {Figured} {Bass} {Approach}},
	shorttitle = {Automatic {Chord} {Labelling}},
	doi = {10.1145/3424911.3425513},
	booktitle = {Proceedings of the 7th {International} {Conference} on {Digital} {Libraries} for {Musicology}},
	author = {Ju, Yaolong and Margot, Sylvain and McKay, Cory and Fujinaga, Ichiro},
	year = {2020},
	note = {GSCC: 0000003 2025-08-12T12:17:55.488Z 0.01
TLDR: This paper proposes a series of four rule-based algorithms that automatically generate chord labels for homorhythmic Baroque chorales based on both figured bass annotations and the musical surface, which are applied to the existing Bach Chorales Figured Bass dataset.},
	pages = {27--31},
	file = {Full Text:C\:\\Users\\Mechrevo\\Zotero\\storage\\WL46WWTQ\\Ju et al. - 2020 - Automatic Chord Labelling A Figured Bass Approach.pdf:application/pdf},
}

@inproceedings{ju_non-chord_2017,
	address = {Shanghai, China},
	title = {Non-chord {Tone} {Identification} {Using} {Deep} {Neural} {Networks}},
	isbn = {978-1-4503-5347-2},
	url = {http://dl.acm.org/citation.cfm?doid=3144749.3144753},
	doi = {10.1145/3144749.3144753},
	abstract = {This demo addresses the problem of harmonic analysis by proposing a non-chord tone identiﬁcation model using deep neural networks (DNNs). By identifying non-chord tones, the task of harmonic analysis is much simpliﬁed. Trained and tested on a dataset of 140 Bach chorales, the DNN model was able to identify non-chord tones with F1measure of 72.19\% using pitch-class, metric information, and a small contextual window around each input sonority as input features. These results suggest that DNNs offer an innovative and promising approach to tackling the problem of non-chord tone identiﬁcation, as well as harmonic analysis.},
	language = {en},
	urldate = {2019-12-06},
	booktitle = {Proceedings of the 4th {International} {Workshop} on {Digital} {Libraries} for {Musicology}  - {DLfM} '17},
	publisher = {ACM Press},
	author = {Ju, Yaolong and Condit-Schultz, Nathaniel and Arthur, Claire and Fujinaga, Ichiro},
	year = {2017},
	note = {GSCC: 0000020 2025-08-12T12:17:45.227Z 0.04
TLDR: The results suggest that DNNs offer an innovative and promising approach to tackling the problem of non-chord tone identification, as well as harmonic analysis.},
	pages = {13--16},
	file = {Ju et al. - 2017 - Non-chord Tone Identification Using Deep Neural Ne.pdf:C\:\\Users\\Mechrevo\\Zotero\\storage\\D63IYG88\\Ju et al. - 2017 - Non-chord Tone Identification Using Deep Neural Ne.pdf:application/pdf},
}

@inproceedings{li_vocemb4svs_2022,
	address = {Chiang Mai, Thailand},
	title = {{VocEmb4SVS}: {Improving} {Singing} {Voice} {Separation} with {Vocal} {Embeddings}},
	isbn = {978-616-590-477-3},
	shorttitle = {{VocEmb4SVS}},
	url = {https://ieeexplore.ieee.org/document/9980293/},
	doi = {10.23919/APSIPAASC55919.2022.9980293},
	abstract = {Deep learning-based methods have shown promising performance on singing voice separation (SVS). Recently, embeddings related to lyrics and voice activities have been proven effective to improve the performance of SVS tasks. However, embeddings related to singers have never been studied before. In this paper, we propose VocEmb4SVS, an SVS framework to utilize vocal embeddings of the singer as auxiliary knowledge for SVS conditioning. First, a pre-trained separation network is employed to obtain pre-separated vocals from the mixed music signals. Second, a vocal encoder is trained to extract vocal embeddings from the pre-separated vocals. Finally, the vocal embeddings are integrated into the separation network to improve SVS performance. Experimental results show that our proposed method achieves state-of-the-art performance on the MUSDB18 dataset with an SDR of 9.56 dB on vocals.},
	language = {en},
	urldate = {2023-05-03},
	booktitle = {2022 {Asia}-{Pacific} {Signal} and {Information} {Processing} {Association} {Annual} {Summit} and {Conference} ({APSIPA} {ASC})},
	publisher = {IEEE},
	author = {Li, Chenyi and Li, Yi and Du, Xuhao and Ju, Yaolong and Hu, Shichao and Wu, Zhiyong},
	month = nov,
	year = {2022},
	note = {GSCC: 0000002 2025-08-12T12:18:11.703Z 0.01
TLDR: VocEmb4SVS is proposed, an SVS framework to utilize vocal embeddings of the singer as auxiliary knowledge for SVS conditioning and achieves state-of-the-art performance on the MUSDB18 dataset.},
	pages = {234--239},
	file = {Li 等 - 2022 - VocEmb4SVS Improving Singing Voice Separation wit.pdf:C\:\\Users\\Mechrevo\\Zotero\\storage\\AUJT56VR\\Li 等 - 2022 - VocEmb4SVS Improving Singing Voice Separation wit.pdf:application/pdf},
}

@inproceedings{ju_interactive_2019,
	title = {An {Interactive} {Workflow} for {Generating} {Chord} {Labels} for {Homorhythmic} {Music} in {Symbolic} {Formats}},
	abstract = {Automatic harmonic analysis is challenging: rule-based models cannot account for every possible edge case, and manual annotation is expensive and sometimes inconsistent, undermining the training and evaluation of machine learning models. We present an interactive workﬂow to address these problems, and test it on Bach chorales. First, a rule-based model was used to generate preliminary, consistent chord labels in order to pre-train three machine learning models. These four models were grouped into an ensemble that generated chord labels by voting, achieving 91.4\% accuracy on a reserved test set. A domain expert then corrected only those chords that the ensemble did not agree on unanimously (20.9\% of the generated labels). Finally, we used these corrected annotations to re-train the machine learning models, and the resulting ensemble attained an accuracy of 93.5\% on the reserved test set, a 24.4\% reduction in the number of errors. This versatile interactive workﬂow can either work in a fully automatic way, or can capitalize on relatively minimal human involvement to generate higher-quality chord labels. It combines the consistency of rule-based models with the nuance of manual analysis to generate relatively inexpensive highquality ground truth for training effective machine learning models.},
	language = {en},
	booktitle = {Proceedings of the 20th {International} {Society} for {Music} {Information} {Retrieval} {Conference}},
	author = {Ju, Yaolong and Howes, Samuel and McKay, Cory and Condit-Schultz, Nathaniel and Calvo-Zaragoza, Jorge and Fujinaga, Ichiro},
	year = {2019},
	note = {GSCC: 0000011 2025-08-12T12:18:22.961Z 0.03},
	keywords = {⛔ No DOI found},
	pages = {862--869},
	file = {Ju et al. - 2019 - AN INTERACTIVE WORKFLOW FOR GENERATING CHORD LABEL.pdf:C\:\\Users\\Mechrevo\\Zotero\\storage\\8XGH2ML8\\Ju et al. - 2019 - AN INTERACTIVE WORKFLOW FOR GENERATING CHORD LABEL.pdf:application/pdf},
}

@inproceedings{ju_automatic_2020-1,
	title = {Automatic {Figured} {Bass} {Annotation} {Using} the {New} {Bach} {Chorales} {Figured} {Bass} {Dataset}},
	booktitle = {Proceedings of the 21th {International} {Society} for {Music} {Information} {Retrieval} {Conference}},
	author = {Ju, Yaolong and Margot, Sylvain and McKay, Cory and Dahn, Luke and Fujinaga, Ichiro},
	year = {2020},
	note = {GSCC: 0000005 2025-08-12T12:17:36.531Z 0.02},
	pages = {640--646},
	file = {Full Text:C\:\\Users\\Mechrevo\\Zotero\\storage\\PDV827VG\\Margot et al. - AUTOMATIC FIGURED BASS ANNOTATION USING THE NEW BA.pdf:application/pdf},
}

@inproceedings{condit-schultz_flexible_2018,
	title = {A {Flexible} {Approach} to {Automated} {Harmonic} {Analysis}: {Multiple} {Annotations} of {Chorales} by {Bach} and {Prætorius}},
	abstract = {Despite being a core component of Western music theory, harmonic analysis remains a subjective endeavor, resistant automation. This subjectivity arises from disagreements regarding, among other things, the interpretation of contrapuntal ﬁgures, the set of “legal” harmonies, and how harmony relates to more abstract features like tonal function. In this paper, we provide a formal speciﬁcation of harmonic analysis. We then present a novel approach to computational harmonic analysis: rather than computing harmonic analyses based on one speciﬁc set of rules, we compute all possible analyses which satisfy only basic, uncontroversial constraints. These myriad interpretations can later be ﬁltered to extract preferred analyses; for instance, to forbid 7th chords or to prefer analyses with fewer non-chord tones. We apply this approach to two concrete musical datasets: existing encodings of 371 chorales by J.S. Bach and new encodings of 200 chorales by M. Prætorius. Through an online API users can ﬁlter and download numerous harmonic interpretations of these 571 chorales. This dataset will serve as a useful resource in the study of harmonic/functional progression, voice-leading, and the relationship between melody and harmony, and as a stepping stone towards automated harmonic analysis of more complex music.},
	booktitle = {Proceedings of the 19th {International} {Society} of {Music} {Information} {Retrieval} {Conference}},
	author = {Condit-Schultz, Nathaniel and Ju, Yaolong and Fujinaga, Ichiro},
	year = {2018},
	note = {GSCC: 0000019 2025-08-12T12:18:26.498Z 0.05},
	pages = {66--73},
	file = {Condit-Schultz et al. - 2018 - A FLEXIBLE APPROACH TO AUTOMATED HARMONIC ANALYSIS.pdf:C\:\\Users\\Mechrevo\\Zotero\\storage\\WEB9IVYJ\\Condit-Schultz et al. - 2018 - A FLEXIBLE APPROACH TO AUTOMATED HARMONIC ANALYSIS.pdf:application/pdf},
}

@inproceedings{ju_figured_2020,
	title = {Figured {Bass} {Encodings} for {Bach} {Chorales} in {Various} {Symbolic} {Formats}: {A} {Case} {Study}},
	shorttitle = {Figured {Bass} {Encodings} for {Bach} {Chorales} in {Various} {Symbolic} {Formats}},
	booktitle = {Proceedings of the {Music} {Encoding} {Conference}},
	author = {Ju, Yaolong and Margot, Sylvain and McKay, Cory and Fujinaga, Ichiro},
	year = {2020},
	note = {GSCC: 0000001 2025-08-12T12:18:39.010Z 0.00},
	keywords = {⛔ No DOI found},
	pages = {71--73},
	file = {music_encoding_conference_proceedings_2020.pdf.pdf:C\:\\Users\\Mechrevo\\Zotero\\storage\\7H6FVRA5\\music_encoding_conference_proceedings_2020.pdf.pdf:application/pdf},
}

@inproceedings{ju_improving_2023,
	title = {Improving {Automatic} {Singing} {Skill} {Evaluation} with {Timbral} {Features}, {Attention}, and {Singing} {Voice} {Separation}},
	url = {https://ieeexplore.ieee.org/abstract/document/10219750?casa_token=xsmU2FvLXmgAAAAA:Kem6hqAiL9zzMC6fSPuxmm2vRQk2JPw3Rh7wOSbxcsjD2yQ1xpFSs_4zjspZYB-lQ663ukmqdnDZaw},
	doi = {10.1109/ICME55011.2023.00111},
	abstract = {Most automatic singing skill evaluation (ASSE) models focus only on solo singing, resulting in a limited application scope since singing is usually mixed with instrumental accompaniment in music. In this paper, we propose a more general ASSE model which applies to both solo singing and singing with accompaniment. For this purpose, we employ an existing singing voice separation tool for accompaniment removal and compare ASSE models trained with and without accompaniment. Results show that accompaniment removal achieves better performances. Furthermore, we explore different features and model architectures, concluding that the additions of timbral features, attention mechanism, and dense layer further improve the performance. Finally, we show that our proposed model achieves a Pearson correlation coefficient of 0.562, a 62.4\% relative improvement compared to 0.346 for the baseline model.},
	urldate = {2025-01-30},
	booktitle = {2023 {IEEE} {International} {Conference} on {Multimedia} and {Expo} ({ICME})},
	author = {Ju, Yaolong and Xu, Chunyang and Guo, Yichen and Li, Jinhu and Lui, Simon},
	month = jul,
	year = {2023},
	note = {GSCC: 0000002 2025-08-12T12:18:15.625Z 0.02 
ISSN: 1945-788X
TLDR: This paper proposes a more general ASSE model which applies to both solo singing and singing with accompaniment, and employs an existing singing voice separation tool for accompaniment removal and compares ASSE models trained with and without accompaniment.},
	keywords = {Instruments, Data augmentation, Data models, singing voice separation, accompaniment removal, attention, Correlation coefficient, Manuals, Timbral features, Videos},
	pages = {612--617},
	file = {Ju 等 - 2023 - Improving Automatic Singing Skill Evaluation with .pdf:C\:\\Users\\Mechrevo\\Zotero\\storage\\EF22TUVD\\Ju 等 - 2023 - Improving Automatic Singing Skill Evaluation with .pdf:application/pdf},
}

@inproceedings{degroot-maggetti_data_2020,
	title = {Data Quality Matters: {Iterative} Corrections on a Corpus of {Mendelssohn} String Quartets and Implications for {MIR} Analysis},
	shorttitle = {Data quality matters},
	url = {https://hal.science/hal-02934884/},
	urldate = {2025-02-05},
	booktitle = {International {Society} for {Music} {Information} {Retrieval} {Conference} ({ISMIR} 2020)},
	author = {Degroot-Maggetti, Jacob and de Reuse, Timothy and Feisthauer, Laurent and Howes, Samuel and Ju, Yaolong and Kokubu, Suzaka and Margot, Sylvain and López, Néstor Nápoles and Upham, Finn},
	year = {2020},
	note = {GSCC: 0000011 2025-08-12T12:18:31.829Z 0.04},
	keywords = {⛔ No DOI found},
	file = {Available Version (via Google Scholar):C\:\\Users\\Mechrevo\\Zotero\\storage\\INRGYHAN\\Degroot-Maggetti et al. - 2020 - Data quality matters Iterative corrections on a corpus of Mendelssohn string quartets and implicati.pdf:application/pdf},
}

@article{huang_k-means_2012,
	title = {K-means initial clustering center optimal algorithm based on {Kruskal}},
	volume = {9},
	url = {https://scholar.google.com/scholar?cluster=6885691506964143339&hl=en&oi=scholarr},
	number = {9},
	urldate = {2025-03-24},
	journal = {J. Inf. Comput. Sci},
	author = {Huang, Lan and Du, Shixian and Zhang, Yu and Ju, Yaolong and Li, Zhuo},
	year = {2012},
	note = {GSCC: 0000005 2025-08-12T12:17:31.193Z 0.01},
	pages = {2387--2392},
	file = {PDF:C\:\\Users\\Mechrevo\\Zotero\\storage\\3YIBGS62\\Huang et al. - 2012 - K-means initial clustering center optimal algorithm based on Kruskal.pdf:application/pdf},
}

@inproceedings{ju_end--end_2024,
	address = {San Francisco, California, USA and Online},
	title = {End-to-{End} {Automatic} {Singing} {Skill} {Evaluation} {Using} {Cross}-{Attention} and {Data} {Augmentation} for {Solo} {Singing} and {Singing} {With} {Accompaniment}},
	url = {https://zenodo.org/records/14877383},
	doi = {10.5281/zenodo.14877383},
	abstract = {Automatic singing skill evaluation (ASSE) systems are predominantly designed for solo singing, and the scenario of singing with accompaniment is largely unaddressed. In this paper, we propose an end-to-end ASSE system that effectively processes both solo singing and singing with accompaniment using data augmentation, where a comparative study is conducted on four different data augmentation approaches. Additionally, we incorporate bi-directional cross-attention (BiCA) for feature fusion which, compared to simple concatenation, can better exploit the inter-relationships between different features. Results on the 10KSinging dataset show that data augmentation and BiCA boost performance individually. When combined, they contribute to further improvements, with a Pearson correlation coefficient of 0.769 for solo singing and 0.709 for singing with accompaniment. This represents relative improvements of 36.8\% and 26.2\% compared to the baseline model score of 0.562, respectively.},
	urldate = {2025-03-24},
	publisher = {ISMIR},
	author = {Ju, Yaolong and Wu, Chun Yat and Lorenzo, Betty Cortiñas and Yang, Jing and Deng, Jiajun and Fan, Fan and Lui, Simon},
	month = nov,
	year = {2024},
	note = {GSCC: 0000000 2025-08-12T12:18:34.987Z 0.00 
    booktitle = {Proceedings of the 25th International Society for Music Information Retrieval Conference},
	pages = {493--500},
	file = {Full Text PDF:C\:\\Users\\Mechrevo\\Zotero\\storage\\ML9RPFZN\\Ju et al. - 2024 - End-to-End Automatic Singing Skill Evaluation Using Cross-Attention and Data Augmentation for Solo S.pdf:application/pdf},
}

@inproceedings{lin_multi-view_2024,
	title = {Multi-{View} {Midivae}: {Fusing} {Track}- and {Bar}-{View} {Representations} for {Long} {Multi}-{Track} {Symbolic} {Music} {Generation}},
	shorttitle = {Multi-{View} {Midivae}},
	url = {https://ieeexplore.ieee.org/document/10448249/},
	doi = {10.1109/ICASSP48485.2024.10448249},
	abstract = {Variational Autoencoders (VAEs) constitute a crucial component of neural symbolic music generation, among which some works have yielded outstanding results and attracted considerable attention. Nevertheless, previous VAEs still encounter issues with overly long feature sequences and generated results lack contextual coherence, thus the challenge of modeling long multi-track symbolic music still remains unaddressed. To this end, we propose Multi-view MidiVAE, as one of the pioneers in VAE methods that effectively model and generate long multi-track symbolic music. The Multi-view MidiVAE utilizes the two-dimensional (2-D) representation, OctupleMIDI, to capture relationships among notes while reducing the feature sequences length. Moreover, we focus on instrumental characteristics and harmony as well as global and local information about the musical composition by employing a hybrid variational encoding-decoding strategy to integrate both Track- and Bar-view MidiVAE features. Objective and subjective experimental results on the CocoChorales dataset demonstrate that, compared to the baseline, Multi-view MidiVAE exhibits significant improvements in terms of modeling long multi-track symbolic music.},
	urldate = {2025-05-25},
	booktitle = {{ICASSP} 2024 - 2024 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	author = {Lin, Zhiwei and Chen, Jun and Tang, Boshi and Sha, Binzhu and Yang, Jing and Ju, Yaolong and Fan, Fan and Kang, Shiyin and Wu, Zhiyong and Meng, Helen},
	month = apr,
	year = {2024},
	note = {GSCC: 0000001 2025-08-12T12:18:04.061Z 0.01 
ISSN: 2379-190X
TLDR: Object and subjective experimental results demonstrate that, compared to the baseline, Multi-view MidiVAE exhibits significant improvements in terms of modeling long multi-track symbolic music.},
	keywords = {Acoustics, Coherence, Instruments, long multi-track, Multi-view MidiVAE, Multiple signal classification, Signal processing, Speech processing, symbolic music generation, Two-dimensional displays},
	pages = {941--945},
	file = {Full Text PDF:C\:\\Users\\Mechrevo\\Zotero\\storage\\2VAI8N3Z\\Lin et al. - 2024 - Multi-View Midivae Fusing Track- and Bar-View Representations for Long Multi-Track Symbolic Music G.pdf:application/pdf},
}

@inproceedings{wu_cycle_2024,
	title = {Cycle {Frequency}-{Harmonic}-{Time} {Transformer} for {Note}-{Level} {Singing} {Voice} {Transcription}},
	url = {https://ieeexplore.ieee.org/document/10687517/},
	doi = {10.1109/ICME57554.2024.10687517},
	abstract = {Singing voice transcription (SVT) is the task of converting singing voice music into symbolic note series. Although most SVT models utilized the time-frequency information from the input spectrogram, the useful harmonic information in singing voices has not been utilized enough. In this paper, we propose a novel 3D Cycle Frequency-Harmonic-Time Transformer (CFT) to explicitly capture the harmonic series of singing voices, where we first define a tokenization scheme that captures harmonics across multiple octaves, then the harmonic features are aggregated into the frequency-harmonic-time representations via a cyclic architecture. Results show that our method achieves state-of-the-art performances on several public datasets, including note-wise accuracy increases of 5.76\% for MIR-ST500 and 13.56\% for Cmedia.},
	urldate = {2025-05-25},
	booktitle = {2024 {IEEE} {International} {Conference} on {Multimedia} and {Expo} ({ICME})},
	author = {Wu, Yulun and Ju, Yaolong and Lui, Simon and Yang, Jing and Fan, Fan and Du, Xuhao},
	month = jul,
	year = {2024},
	note = {GSCC: 0000000 2025-08-12T12:18:17.609Z 0.00 
ISSN: 1945-788X
TLDR: A novel 3D Cycle Frequency-Harmonic-Time Transformer (CFT) is proposed to explicitly capture the harmonic series of singing voices, where a tokenization scheme is defined that captures harmonics across multiple octaves, then the harmonic features are aggregated into the frequency-harmonic-time representations via a cyclic architecture.},
	keywords = {Harmonic analysis, Time-frequency analysis, Training, automatic music transcription, music information retrieval, Transformers, note-level singing voice transcription, Solid modeling, Three-dimensional displays, Tokenization},
	pages = {1--6},
	file = {Full Text PDF:C\:\\Users\\Mechrevo\\Zotero\\storage\\D58MKQMF\\Wu et al. - 2024 - Cycle Frequency-Harmonic-Time Transformer for Note-Level Singing Voice Transcription.pdf:application/pdf},
}

@inproceedings{deng_efficient_2024,
	title = {Efficient adapter tuning for joint singing voice beat and downbeat tracking with self-supervised learning features},
	url = {https://doi.org/10.5281/zenodo.14877345},
	doi = {10.5281/zenodo.14877345},
	publisher = {ISMIR},
	booktitle = {Proceedings of the 25th International Society for Music Information Retrieval Conference},
	author = {Deng, Jiajun and Ju, Yaolong and Yang, Jing and Lui, Simon and Liu, Xunying},
	month = nov,
	year = {2024},
	note = {TLDR: A novel temporal convolutional network-based beat-tracking approach featuring self-supervised learning representations and adapter tuning is proposed to track the beat and downbeat of singing voices jointly.},
	pages = {343--351},
	file = {EfficientAdapter_FinalVersion (1):C\:\\Users\\Mechrevo\\Zotero\\storage\\SPLJ36DJ\\EfficientAdapter_FinalVersion (1).pdf:application/pdf},
}

@phdthesis{ju_addressing_2021,
	title = {Addressing ambiguity in supervised machine learning: {A} case study on automatic chord labelling},
	url = {https://www.proquest.com/openview/5ab3e6bea1a90fd9b065c6826abc318c/1?pq-origsite=gscholar&cbl=18750&diss=y},
	abstract = {Chord labelling is an important analytical tool in Western tonal music with many possible applications. Manual chord labelling is a time-consuming process, and automatic chord labelling can be a promising alternative. However, most automated approaches to date have not sufficiently considered ambiguity in chord labelling, where multiple answers are possible due to different labelling strategies. In this dissertation, I present three ways of addressing this ambiguity in automatic chord labelling, using J. S. Bach’s chorales as a case study.},
	language = {en},
	school = {McGill University},
	author = {Ju, Yaolong},
	year = {2021},
	keywords = {⛔ No DOI found},
	file = {PDF:C\:\\Users\\Mechrevo\\Zotero\\storage\\S646EMV4\\Ju - Addressing ambiguity in supervised machine learning A case study on automatic chord labelling.pdf:application/pdf},
}

@misc{zhou_animetab_2022,
	title = {{AnimeTAB}: {A} new guitar tablature dataset of anime and game music},
	shorttitle = {{AnimeTAB}},
	url = {http://arxiv.org/abs/2210.03027},
	doi = {10.48550/arXiv.2210.03027},
	abstract = {While guitar tablature has become a popular topic in MIR research, there exists no such a guitar tablature dataset that focuses on the soundtracks of anime and video games, which have a surprisingly broad and growing audience among the youths. In this paper, we present AnimeTAB, a fingerstyle guitar tablature dataset in MusicXML format, which provides more high-quality guitar tablature for both researchers and guitar players. AnimeTAB contains 412 full tracks and 547 clips, the latter are annotated with musical structures (intro, verse, chorus, and bridge). An accompanying analysis toolkit, TABprocessor, is included to further facilitate its use. This includes functions for melody and bassline extraction, key detection, and chord labeling, which are implemented using rule-based algorithms. We evaluated each of these functions against a manually annotated ground truth. Finally, as an example, we performed a music and technique analysis of AnimeTAB using TABprocessor. Our data and code have been made publicly available for composers, performers, and music information retrieval (MIR) researchers alike.},
	urldate = {2025-08-20},
	publisher = {arXiv},
	author = {Zhou, Yuecheng and Ju, Yaolong and Xie, Lingyun},
	month = oct,
	year = {2022},
	note = {arXiv:2210.03027 [cs]
TLDR: This paper presents AnimeTAB, a fingerstyle Guitar tablature dataset in MusicXML format, which provides more high-quality guitar tablature for both researchers and guitar players and an accompanying analysis toolkit, TABprocessor, is included to further facilitate its use.},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	file = {Preprint PDF:C\:\\Users\\Mechrevo\\Zotero\\storage\\AVEUHRRB\\Zhou et al. - 2022 - AnimeTAB A new guitar tablature dataset of anime and game music.pdf:application/pdf;Snapshot:C\:\\Users\\Mechrevo\\Zotero\\storage\\4PLHIP85\\2210.html:text/html},
}

@misc{tsoi_crossmusim_2025,
	title = {{CrossMuSim}: {A} cross-modal framework for music similarity retrieval with {LLM}-powered text description sourcing and mining},
	shorttitle = {{CrossMuSim}},
	url = {http://arxiv.org/abs/2503.23128},
	doi = {10.48550/arXiv.2503.23128},
	abstract = {Music similarity retrieval is fundamental for managing and exploring relevant content from large collections in streaming platforms. This paper presents a novel cross-modal contrastive learning framework that leverages the open-ended nature of text descriptions to guide music similarity modeling, addressing the limitations of traditional uni-modal approaches in capturing complex musical relationships. To overcome the scarcity of high-quality text-music paired data, this paper introduces a dual-source data acquisition approach combining online scraping and LLM-based prompting, where carefully designed prompts leverage LLMs' comprehensive music knowledge to generate contextually rich descriptions. Exten1sive experiments demonstrate that the proposed framework achieves significant performance improvements over existing benchmarks through objective metrics, subjective evaluations, and real-world A/B testing on the Huawei Music streaming platform.},
	urldate = {2025-08-20},
	publisher = {arXiv},
	author = {Tsoi, Tristan and Deng, Jiajun and Ju, Yaolong and Weck, Benno and Kirchhoff, Holger and Lui, Simon},
	month = may,
	year = {2025},
	note = {arXiv:2503.23128 [cs]
TLDR: This paper introduces a dual-source data acquisition approach combining online scraping and LLM-based prompting, where carefully designed prompts leverage LLMs' comprehensive music knowledge to generate contextually rich descriptions.},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	annote = {Comment: Accepted by ICME2025},
	file = {Preprint PDF:C\:\\Users\\Mechrevo\\Zotero\\storage\\3LZPKX3K\\Tsoi et al. - 2025 - CrossMuSim A cross-modal framework for music similarity retrieval with LLM-powered text description.pdf:application/pdf;Snapshot:C\:\\Users\\Mechrevo\\Zotero\\storage\\M94FHP2D\\2503.html:text/html},
}
